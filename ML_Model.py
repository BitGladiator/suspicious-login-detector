# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S982SlPocPW0zpKRaqenxF96euCTAOQV
"""

!pip install -q kaggle pandas scikit-learn joblib python-dotenv

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle --version

DATASET_SLUG="dasgroup/rba-dataset"
!mkdir -p /content/data
!kaggle datasets download -d $DATASET_SLUG -p /content/data --unzip

import glob, os
files = glob.glob('/content/data/**', recursive=True)
for f in files[:200]:
    print(f)

"""## Load Data Set"""

import os
import glob
import pandas as pd

csv_path = "/content/data/rba-dataset.csv"  # Path where dataset is downloaded

# Check if file exists; if not, search any CSV recursively in /content/data
if not os.path.exists(csv_path):
    csvs = [p for p in glob.glob('/content/data/**/*.csv', recursive=True)]
    if len(csvs) > 0:
        csv_path = csvs[0]
    else:
        raise FileNotFoundError("No CSV found in /content/data - check downloaded files")

print("Dataset file found:", csv_path)

# Check file size for reference
!ls -lh "{csv_path}"

# Load a small sample to verify file is readable and not corrupted
print("\nLoading a small sample of 1000 rows...")
df_sample = pd.read_csv(csv_path, nrows=1000)
print("Sample shape:", df_sample.shape)
print(df_sample.head())

# Load full dataset in chunks to avoid memory/time issues
print("\nLoading full dataset in chunks...")
chunk_iter = pd.read_csv(csv_path, chunksize=50000)  # Adjust chunksize as needed
df_chunks = []

for i, chunk in enumerate(chunk_iter):
    print(f"Loaded chunk {i+1} with shape {chunk.shape}")
    df_chunks.append(chunk)

df = pd.concat(df_chunks, ignore_index=True)
print("\nFull dataset loaded successfully.")
print("Full dataset shape:", df.shape)

import os
import glob
import numpy as np
import pandas as pd
from IPython.display import display

candidate_paths = []
if 'csv_path' in globals():
    candidate_paths.append(csv_path)

candidate_paths += [
    '/content/data/rba-dataset.csv',
    '/content/data/train.csv',
    '/content/data/rba-dataset/*.csv',
    '/content/data/**/*.csv',
    '/content/*.csv',

    './*.csv'
]

found = []
for p in candidate_paths:
    # if pattern contains wildcard use glob
    if any(ch in p for ch in ['*', '?', '[']):
        found += glob.glob(p, recursive=True)
    else:
        if os.path.exists(p):
            found.append(p)

if len(found) == 0:
    found = glob.glob('/content/**/**/*.csv', recursive=True) + glob.glob('/content/*.csv')

found = list(dict.fromkeys(found))
if len(found) == 0:
    raise FileNotFoundError("No CSV found in /content or known candidate paths. Upload the dataset to /content/data or set csv_path variable.")

csv_path = found[0]
print("Using dataset file:", csv_path)
print("File info:")
!ls -lh "{csv_path}"

try:
    print("\nReading a small sample (1000 rows) to detect columns...")
    sample = pd.read_csv(csv_path, nrows=1000)
    print("Sample columns:", sample.columns.tolist())
except Exception as e:
    raise RuntimeError(f"Failed to read sample of CSV: {e}")

# Decide whether to load full file into memory or use a sample based on file size
file_size_bytes = os.path.getsize(csv_path)
mb = file_size_bytes / (1024*1024)
print(f"\nFile size: {mb:.2f} MB")

if mb < 300:
    print("Loading full CSV into memory (file < 300MB)...")
    df = pd.read_csv(csv_path, low_memory=False)
else:
    print("Large file detected â€” loading in chunks and concatenating (adjust chunksize if needed)...")
    chunksize = 100000  # adjust if necessary
    it = pd.read_csv(csv_path, chunksize=chunksize, low_memory=False)
    parts = []
    for i, chunk in enumerate(it):
        print(f"  loaded chunk {i+1} shape={chunk.shape}")
        parts.append(chunk)
    df = pd.concat(parts, ignore_index=True)
    print("Concatenated chunks. Total rows:", len(df))

print("\nLoaded DataFrame shape:", df.shape)
display(df.head())

ts_col = None
if 'timestamp' in df.columns:
    ts_col = 'timestamp'
else:
    for c in df.columns:
        if ('time' in c.lower()) or ('date' in c.lower()):
            ts_col = c
            break

if ts_col is not None:
    df['timestamp'] = pd.to_datetime(df[ts_col], errors='coerce')
else:
    df['timestamp'] = pd.NaT

if df['timestamp'].isna().sum() > 0.5 * len(df):
    start = pd.Timestamp("2025-01-01")
    df['timestamp'] = start + pd.to_timedelta(np.cumsum(np.random.randint(60,600,size=len(df))), unit='s')
else:
    df['timestamp'] = df['timestamp'].fillna(method='ffill').fillna(method='bfill')
    if df['timestamp'].isna().all():
        start = pd.Timestamp("2025-01-01")
        df['timestamp'] = start + pd.to_timedelta(np.arange(len(df)), unit='s')

user_col = None
for c in df.columns:
    if c.lower() in ['user','user_id','username','uid','userid','account']:
        user_col = c
        break
if user_col is None:
    df['user_id'] = (df.index % 1000) + 1
    user_col = 'user_id'

ip_col = None
for c in df.columns:
    if 'ip' in c.lower() and df[c].dtype == object:
        ip_col = c
        break
if ip_col is None:
    df['ip'] = '0.0.0.' + (df.index % 255).astype(str)
    ip_col = 'ip'

device_col = None
for c in df.columns:
    if any(k in c.lower() for k in ['device','ua','useragent','user_agent','browser']):
        device_col = c
        break
if device_col is None:
    df['device_id'] = df[ip_col].astype(str) + "_" + df[user_col].astype(str)
    device_col = 'device_id'
    success_col = None
for c in df.columns:
    if any(k in c.lower() for k in ['success','status','result','outcome','is_success']):
        success_col = c
        break

if success_col is not None:
    df['success'] = df[success_col].apply(lambda x: 1 if str(x).lower() in ['true','success','ok','1','passed'] else 0)
else:
    if 'label' in df.columns:
        df['success'] = (df['label'] == 0).astype(int)
    else:
        df['success'] = 1

